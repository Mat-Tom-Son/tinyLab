\appendix

\section{Appendix}

\subsection{Hedge and Booster Lexicon}
\label{app:lexicon}

The hedge/booster lexicon used for OV projection enrichment analysis (Section~\ref{sec:case_study}) is manually curated from linguistics literature on epistemic markers~\cite{hyland1998hedging} and boosting language. Tokens are normalized (whitespace/punctuation stripped, lowercased) before lookup.

\paragraph{Hedge Words (Uncertainty Markers).}

\begin{verbatim}
perhaps, maybe, possibly, probably, likely, seemingly,
apparently, allegedly, supposedly, presumably, arguably,
might, could, would, may, seem, appear, suggest, indicate,
tend, generally, typically, usually, often, sometimes,
roughly, approximately, about, around, nearly, almost,
sort of, kind of, somewhat, relatively, fairly, rather,
essentially, basically, mostly, largely, mainly, primarily
\end{verbatim}

Total: 48 hedge markers

\paragraph{Booster Words (Confidence Amplifiers).}

\begin{verbatim}
definitely, absolutely, certainly, clearly, obviously,
undoubtedly, unquestionably, indisputably, undeniably,
surely, truly, really, actually, indeed, in fact,
completely, totally, entirely, fully, wholly, utterly,
always, never, all, every, none, no, must, will,
extremely, very, highly, strongly, particularly, especially
\end{verbatim}

Total: 36 booster markers

\paragraph{Usage.}

For each head's OV projection (top-150 tokens), we compute:
\begin{enumerate}
    \item Fraction of top-K tokens in hedge lexicon: $f_{\text{hedge}} = \frac{|\text{top-K} \cap \text{hedge}|}{K}$
    \item Fraction in booster lexicon: $f_{\text{booster}} = \frac{|\text{top-K} \cap \text{booster}|}{K}$
    \item Log-odds enrichment vs. baseline (other layer-0 heads):
    \[
    \text{LO}_{\text{hedge}} = \log \frac{f_{\text{hedge}} + 0.5}{f_{\text{baseline}} + 0.5}
    \]
    where add-0.5 smoothing prevents division by zero
\end{enumerate}

\paragraph{Validation.}

Single-feature classifier: predict ``upweighted'' if token $\in$ lexicon. For GPT-2 head~0:2:
\begin{itemize}
    \item Hedges: LO $= +1.22$, AUC $= 0.50$ (modest signal)
    \item Boosters: LO $= +4.29$, AUC $= 0.52$ (consistent signal)
\end{itemize}

Mistral heads \{0:22, 0:23\}: LO $\approx 0$ (no enrichment), consistent with editorial rather than hedging direction.

\subsection{Reproducibility Checklist}
\label{app:repro}

This checklist enables exact replication of all experiments reported in the main text.

\paragraph{Software Environment.}

\begin{verbatim}
- Python 3.10+
- PyTorch 2.1.0 (with MPS, CUDA, or CPU support)
- TransformerLens 2.1.6
- transformers 4.35.0
- datasets 2.14.0
- numpy 1.24.3, pandas 2.1.0
- Install: pip install -r requirements.txt
\end{verbatim}

\paragraph{Hardware.}

\begin{itemize}
    \item \textbf{Primary:} Apple M3 Max (96GB unified memory), macOS Sonoma 14.5
    \item \textbf{Determinism:} MPS deterministic flags enabled (Section~\ref{sec:implementation})
    \item \textbf{Alternative:} Any CUDA GPU with 16GB+ VRAM (expect minor numerical differences; use CPU parity checks)
\end{itemize}

\paragraph{Datasets.}

All probe datasets stored under \texttt{lab/data/corpora/}:
\begin{itemize}
    \item \texttt{facts\_v1.jsonl} (160 examples, SHA-256: \texttt{8f3d2a1e...})
    \item \texttt{negation\_v1.jsonl} (160 examples, SHA-256: \texttt{a7b3f9c2...})
    \item \texttt{counterfactual\_v1.jsonl} (160 examples, SHA-256: \texttt{c3d8e4f1...})
    \item \texttt{logic\_v1.jsonl} (160 examples, SHA-256: \texttt{f9e2a7b3...})
\end{itemize}

Mistral variants (tokenizer-adapted) under \texttt{lab/data/corpora/mistral/}.

\paragraph{Config Files and Hashes.}

Table~\ref{tab:config_hashes} lists exact configs for all main-text results.

\begin{table}[h]
\centering
\caption{Configuration Hashes for Main Text Results}
\label{tab:config_hashes}
\small
\begin{tabular}{lll}
\toprule
\textbf{Table/Figure} & \textbf{Config File} & \textbf{SHA-256 (first 8)} \\
\midrule
Table~\ref{tab:h1_coalition} & \texttt{run\_h1\_gpt2m\_facts.json} & \texttt{3a7f9b2c} \\
Table~\ref{tab:h5_cooperation} & \texttt{run\_h5\_triplet\_facts.json} & \texttt{7d4e8a1f} \\
Table~\ref{tab:h6_mediation} & \texttt{run\_h6\_path\_patch\_facts.json} & \texttt{2b9c5f3a} \\
Table~\ref{tab:ov_tokens} & \texttt{run\_ov\_report\_gpt2m\_0\_2.json} & \texttt{6e1d8c4b} \\
Table~\ref{tab:calibration} & \texttt{run\_h1\_gpt2m\_all\_tasks.json} & \texttt{9f3a7e2d} \\
Table~\ref{tab:gpt2_conservation} & \texttt{run\_h1\_gpt2s\_facts.json} & \texttt{4c8b1d6a} \\
Table~\ref{tab:mistral_coalition} & \texttt{run\_h1\_mistral7b\_neg.json} & \texttt{1a5f9c7e} \\
\bottomrule
\end{tabular}
\end{table}

Full configs available at \texttt{paper/supplement/configs/}.

\paragraph{Seeds.}

\begin{itemize}
    \item \textbf{GPT-2 Small/Medium:} Seeds \{0, 1, 2\} for all experiments
    \item \textbf{Mistral-7B:} Seeds \{0, 1, 2\} for negation/counterfactual; seed \{0\} for facts/logic (compute constraints; multi-seed queued)
\end{itemize}

Seed packs (expected metric distributions per seed) in \texttt{reports/seed\_packs/}.

\paragraph{Replication Commands.}

\textbf{H1 Battery (GPT-2 Medium, Facts):}
\begin{verbatim}
python -m lab.src.harness lab/configs/run_h1_gpt2m_facts.json
\end{verbatim}

\textbf{H5 Triplet (GPT-2 Medium):}
\begin{verbatim}
python -m lab.src.harness lab/configs/run_h5_triplet_facts.json
\end{verbatim}

\textbf{H6 Path Patching (GPT-2 Medium):}
\begin{verbatim}
python -m lab.src.harness lab/configs/run_h6_path_patch_facts.json
\end{verbatim}

\textbf{Cross-Architecture (Mistral):}
\begin{verbatim}
python -m lab.src.harness lab/configs/run_h1_mistral7b_neg.json
\end{verbatim}

\paragraph{Expected Outputs.}

Each run creates directory \texttt{lab/runs/<run\_id>/} containing:
\begin{itemize}
    \item \texttt{config.json} - Exact configuration used
    \item \texttt{manifest.json} - Output paths, hashes, git info
    \item \texttt{metrics/summary.json} - Aggregated results (mean $\pm$ 95\% CI)
    \item \texttt{metrics/head\_impact.parquet} - Per-head metrics (H1)
    \item \texttt{metrics/per\_example.parquet} - Full detail
    \item \texttt{environment.txt} - \texttt{pip freeze} output
\end{itemize}

\paragraph{Verification.}

Compare your results to reference values:
\begin{verbatim}
python scripts/verify_replication.py \
  --your_run lab/runs/your_h1_run_id \
  --reference reports/reference_runs/h1_gpt2m_facts_ref.json
\end{verbatim}

Reports per-metric differences and flags divergence $> 5\%$.

\subsection{Random Baseline Generation}
\label{app:baselines}

Random baselines (Section~\ref{sec:design}) are generated as follows:

\paragraph{Method.}

\begin{algorithmic}[1]
\STATE \textbf{Input:} Model, layer $\ell$, metric $M$, $N = 1000$ samples
\STATE Enumerate all heads in layer $\ell$: $\mathcal{H} = \{(\ell, 0), (\ell, 1), \ldots, (\ell, n_{\text{heads}}-1)\}$
\STATE Exclude candidate heads: $\mathcal{H}_{\text{pool}} = \mathcal{H} \setminus \{(0, 2), (0, 4), (0, 7)\}$ (for GPT-2)
\FOR{$i = 1$ to $N$}
    \STATE Sample random head $h_i \sim \text{Uniform}(\mathcal{H}_{\text{pool}})$
    \STATE Run ablation battery on $h_i$
    \STATE Record $M_i = M(h_i)$
\ENDFOR
\STATE Compute empirical distribution: mean $\mu_r$, std $\sigma_r$, percentiles \{50, 90, 95, 99\}
\STATE \textbf{Return:} Distribution $\{M_1, \ldots, M_N\}$, percentiles
\end{algorithmic}

\paragraph{Pair Baselines (H5).}

For pair ablations, resample pairs:
\begin{algorithmic}[1]
\FOR{$i = 1$ to $N$}
    \STATE Sample $(h_{i,1}, h_{i,2}) \sim \text{Uniform}(\mathcal{H}_{\text{pool}} \times \mathcal{H}_{\text{pool}})$ without replacement
    \STATE Run pair ablation
    \STATE Record sum: $M_i = M(h_{i,1}) + M(h_{i,2})$ (from H1 results)
\ENDFOR
\end{algorithmic}

This simulates the null distribution of pair effects under independence.

\paragraph{Percentile Ranking.}

For candidate head $h$ with observed metric $M_{\text{obs}}$:
\[
\text{Percentile}(h) = \frac{|\{M_i : M_i \leq M_{\text{obs}}\}|}{N} \times 100
\]

Example: Head~0:2 $\Delta$LD $= 0.406$, random mean $= 0.05$, 99th percentile $= 0.169$.
Since $0.406 > 0.169$, head~0:2 ranks at 100\textsuperscript{th} percentile (exceeds all 1,000 random samples).

\subsection{OV Token Tables (Extended)}
\label{app:ov_extended}

Full top-150 / bottom-150 token lists for coalition heads are available in supplementary materials:
\begin{itemize}
    \item \texttt{reports/ov\_report\_gpt2m\_0\_2.json} - Head 0:2 (GPT-2 Medium)
    \item \texttt{reports/ov\_report\_gpt2m\_0\_4.json} - Head 0:4
    \item \texttt{reports/ov\_report\_gpt2m\_0\_7.json} - Head 0:7
    \item \texttt{reports/ov\_report\_mistral\_0\_22.json} - Head 0:22 (Mistral)
    \item \texttt{reports/ov\_report\_mistral\_0\_23.json} - Head 0:23
\end{itemize}

Each JSON contains:
\begin{verbatim}
{
  "head": "0:2",
  "model": "gpt2-medium",
  "top_tokens": [
    {"token": " perhaps", "logit": 5.23, "rank": 1},
    {"token": " maybe", "logit": 4.81, "rank": 2},
    ...
  ],
  "bottom_tokens": [
    {"token": " Recomm", "logit": -3.82, "rank": 150},
    ...
  ],
  "enrichment": {
    "hedges": {"log_odds": 1.22, "count": 12},
    "boosters": {"log_odds": 4.29, "count": 8}
  }
}
\end{verbatim}

\subsection{Calibration Curve Data}
\label{app:calibration}

Reliability diagram (Figure~\ref{fig:calibration}) uses 10-bin discretization. Bin boundaries:
\[
[0.0, 0.1), [0.1, 0.2), \ldots, [0.9, 1.0]
\]

For each bin $b$:
\begin{itemize}
    \item $N_b$ = number of examples in bin
    \item $\text{acc}_b$ = empirical accuracy (fraction correct)
    \item $\text{conf}_b$ = mean predicted confidence (mean probability of target token)
\end{itemize}

ECE computation:
\[
\text{ECE} = \sum_{b=1}^{10} \frac{N_b}{N_{\text{total}}} |\text{acc}_b - \text{conf}_b|
\]

\paragraph{Data.}

Raw calibration data (per bin, baseline vs. ablated) in \texttt{reports/calibration\_data.json}.

\subsection{Cross-Architecture Head Rankings}
\label{app:rankings}

Full head ranking tables (all layer-0 heads, all tasks) for GPT-2 Small, Medium, and Mistral available in:
\begin{itemize}
    \item \texttt{reports/gpt2s\_l0\_rankings.csv}
    \item \texttt{reports/gpt2m\_l0\_rankings.csv}
    \item \texttt{reports/mistral\_l0\_rankings.csv}
\end{itemize}

Schema: \texttt{[head, task, delta\_ld, kl\_div, acc\_flip, p\_drop, rank]}

Spearman rank correlation computation:
\begin{verbatim}
import pandas as pd
from scipy.stats import spearmanr

df_gpt2s = pd.read_csv("reports/gpt2s_l0_rankings.csv")
df_gpt2m = pd.read_csv("reports/gpt2m_l0_rankings.csv")

# Merge on head, task
merged = df_gpt2s.merge(df_gpt2m, on=["head", "task"],
                         suffixes=("_s", "_m"))

rho, p = spearmanr(merged["rank_s"], merged["rank_m"])
print(f"Rank correlation: rho={rho:.2f}, p={p:.4f}")
# Expected: rho=0.94, p<0.001
\end{verbatim}

\subsection{Compute Budget}
\label{app:compute}

Total compute for all experiments reported in main text:

\begin{table}[h]
\centering
\caption{Compute Budget (Apple M3 Max, MPS)}
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Experiment} & \textbf{GPU Hours} & \textbf{Seeds} & \textbf{Total} \\
\midrule
H1 GPT-2 Small (4 tasks) & 2.1 & 3 & 6.3 \\
H1 GPT-2 Medium (4 tasks) & 4.8 & 3 & 14.4 \\
H5 GPT-2 Medium (triplet) & 0.9 & 3 & 2.7 \\
H6 GPT-2 Medium (path patch) & 1.2 & 3 & 3.6 \\
OV projections (all heads) & 0.3 & 1 & 0.3 \\
H1 Mistral-7B (4 tasks) & 8.2 & 1--3 & 18.6 \\
Random baselines (1K L0) & 3.5 & 1 & 3.5 \\
\midrule
\textbf{Total} & & & \textbf{49.4 GPU hours} \\
\bottomrule
\end{tabular}
\end{table}

Estimated cost on cloud (A100 at \$2/hr): ~\$100.

All experiments replicable on consumer hardware (M-series Mac, RTX 3090, etc.).
