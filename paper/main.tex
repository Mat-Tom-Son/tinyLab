\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

\title{Layer-0 Suppressors Ground Hallucination Inevitability:\\A Mechanistic Account of How Transformers Trade Factuality for Hedging}
\author{Mat Thompson\\Independent Researcher, Raleigh, NC}
\date{October 29, 2025}

\begin{document}

\maketitle

\begin{abstract}
We predicted that circuits implementing the factuality–hedging trade-off should emerge at layer~0—the first major information bottleneck—before conducting any experiments. We then validated that prediction across models and probes. Zeroing GPT-2 Medium heads $\{0{:}2,0{:}4,0{:}7\}$ raises logit difference (LD) by $0.40{-}0.85$ and improves expected calibration error from $0.122$ to $0.091$. Lexicon enrichment shows head $0{:}2$ upweights boosters ($+4.29$ log-odds) while demoting factual stems, and random layer-0 ablations confirm the trio lies in the $>99$\textsuperscript{th} percentile tail. Path patching reveals that $67\%$ of the head $0{:}2$ effect is mediated by the suppressor$\rightarrow$layer-11 residual stream, aligning causal structure with the hallucination inevitability theorem of Kalai et~al.\ (2025). Mistral-7B discovers an architecture-adapted variant: heads $\{0{:}22,0{:}23\}$ suppress factual tokens without hedging boosts and are opposed on logic by head $0{:}21$. These results bridge statistical incentives and concrete circuits, motivating suppressor-aware interventions for truthful model behavior.
\end{abstract}

\paragraph{Contributions.}
\begin{itemize}
    \item \textbf{Prediction-first validation.} We formulate a bottleneck prediction (layer~0) from information geometry plus Kalai et~al.'s constraint, then validate it with falsification-oriented tests rather than post-hoc discovery.
    \item \textbf{Rigorous methodology.} Dual observables (power: $\Delta$LD; information: calibration) improve together; empirical random baselines place suppressors in the $>99$\% tail; results replicate across architectures (GPT-2, Mistral).
    \item \textbf{Causal structure.} Forward/reverse path patching shows the suppressor$\rightarrow$layer-11 residual stream mediates $67\%$ of the head $0{:}2$ effect, providing an operational attractor.
    \item \textbf{Open reproducibility.} We ship configs, seeds, hashes, and standardized reports (manifest, rankings, OV tables) to enable detailed review and reuse.
\end{itemize}

\input{sections/introduction}
\input{sections/background}
\input{sections/related_work}
\input{sections/methods}
\input{sections/findings}
\input{sections/interpretation}
\input{sections/implications}
\input{sections/discussion}
\input{sections/future}
\input{sections/conclusion}
\input{sections/appendix}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
