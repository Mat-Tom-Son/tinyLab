\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

\title{Layer-0 Suppressors Ground Hallucination Inevitability:\\A Mechanistic Account of How Transformers Trade Factuality for Hedging}
\author{Mat Thompson\\Independent Researcher, Raleigh, NC}
\date{October 29, 2025}

\begin{document}

\maketitle

\begin{abstract}
Language models must choose: assert confidently, or hedge when uncertain. But where in the network is this trade-off implemented? Drawing on information bottlenecks and Kalai et~al.'s hallucination inevitability theorem, we predicted that the circuits mediating this trade-off would emerge at layer~0—the model’s narrowest and earliest point of compression. We validate this prediction in GPT-2 and Mistral-7B, identifying a small coalition of “layer-0 suppressor” heads that dampen factual continuations and boost hedging or editorial tokens.

Ablating suppressors improves factual preference (logit difference: increase in model preference for the correct token over a matched foil, $+0.40$–$0.85$), calibration (expected calibration error $0.122\rightarrow0.091$), and sequence quality across tasks. Randomized ablations confirm these heads lie in the $>99$\textsuperscript{th}-percentile tail. Causal tracing shows that $67\%$ of suppressor influence flows through a single early-to-mid path (layer~0$\rightarrow$layer~11), forming a stable hedging attractor that downstream layers do not reverse. Suppressors emerge early in training and adapt to architecture—GPT-2 couples hedging boosts with factual suppression, while Mistral separates these functions and introduces a task-contingent anti-suppressor.

These findings provide a mechanistic account of how transformers instantiate a statistical trade-off between truth and caution. Beyond power and calibration, we observe clear geometric signatures under suppressor ablation: output distribution flattening reverses across all four probe families (facts/counterfactual/negation/logic; $\Delta H = -2.4$ to $-3.8$~nats, $p<0.02$) and early trajectory curvature decreases (straighter paths at layer~0). Evaluation reform alone may not eliminate hallucinations: suppressors crystallize at the earliest layers, biasing computation toward qualified language even when knowledge is available. We present evidence consistent with constrained early‑layer solutions at the bottleneck—predictable from geometry and incentives—while allowing that implementation varies by model and data; and we demonstrate a minimal, task-scoped steering intervention.
\end{abstract}

\paragraph{Contributions.}
\begin{itemize}
    \item \textbf{Prediction-first validation.} We formulate a bottleneck prediction (layer~0) from information geometry plus Kalai et~al.'s constraint, then validate it with falsification-oriented tests rather than post-hoc discovery.
    \item \textbf{Rigorous methodology.} Dual observables (power: $\Delta$LD; information: calibration) improve together; empirical random baselines place suppressors in the $>99$\% tail; results replicate across architectures (GPT-2, Mistral).
    \item \textbf{Geometric validation.} Output entropy and trajectory curvature confirm the predicted operation (output flattening, early trajectory bending), replicating across factual, counterfactual, negation, and logic probes.
    \item \textbf{Predictive framework.} We connect training methodology (pretraining, RLHF, Constitutional AI) to suppressor structure via information-theoretic constraints, and pose testable predictions for how different objectives modify the mechanism.
    \item \textbf{Causal structure.} Forward/reverse path patching shows the suppressor$\rightarrow$layer-11 residual stream mediates $67\%$ of the head $0{:}2$ effect, providing an operational attractor.
    \item \textbf{Open reproducibility.} We ship configs, seeds, hashes, and standardized reports (manifest, rankings, OV tables) to enable detailed review and reuse.
\end{itemize}

\input{sections/introduction}
\input{sections/background}
\input{sections/related_work}
\input{sections/methods}
\input{sections/findings}
\input{sections/interpretation}
\input{sections/implications}
\input{sections/discussion}
\input{sections/future}
\input{sections/conclusion}
\input{sections/appendix}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
