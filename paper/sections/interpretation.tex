\section{Mechanistic Interpretation of Suppressor Attractors}

The standard ablation story ends with “remove bad heads, performance improves’’.
Suppressors suggest a richer picture.
When the suppressor trio fires in GPT-2---or the {22, 23} pair in Mistral---the residual stream exiting layer~0 already contains a hedging-oriented rotation of token probabilities.
Downstream attention and feedforward blocks therefore operate in a regime where plausible meta-commentary is pre-selected, making it costly for later layers to reintroduce factual certainty.
Reverse-patch experiments support this attractor view: inserting clean activations into an ablated run does not restore factuality, yet inserting corrupted suppressor activations into a clean run rapidly induces hedging.
Like starting in the wrong lane on a highway, a layer‑0 bias forces later layers to spend capacity changing lanes; correction is possible but costly, so the early hedging trajectory tends to persist.
Figure~\ref{fig:path-dag} summarises the mediated contribution on the facts probe: ablation alone yields $\Delta\mathrm{LD}=+0.40$, reinstating only the suppressor$\rightarrow$layer-11 path leaves $\Delta\mathrm{LD}=+0.13$, so $67\%$ of the effect is mediated by that path; the reciprocal reverse patch drives $\Delta\mathrm{LD}=-0.93$ in the clean model.

In GPT-2, the semantic direction couples suppression and hedging: factual stems are demoted while hedging vocabulary is promoted.
This produces an attractor that favors calibrated-sounding evasions.
Mistral takes a different route.
The suppressor pair demotes factual tokens without a corresponding hedging boost; instead it surfaces multilingual editorial fragments.
The anti-suppressor head~0:21 then selectively counteracts suppression on logic tasks, proving that the attractor is task-contingent rather than globally enforced.

These dynamics align with Kalai et al.'s incentive view.
Suppressors are the concrete machinery that allows a model to satisfy conflicting objectives: keep accuracy high when knowledge is certain, yet emit fluent hedging when knowledge is sparse.
Rather than toggling individual token probabilities late in the computation, the model enters a behavioral basin from which hedged discourse feels natural.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/path_patch_dag.pdf}
    \caption{\textbf{Attractor mediation via path patching.} Forward patch (reinstated L0$\rightarrow$L11 path): $\Delta$LD $= +0.13$; Reverse patch at L11: $\Delta$LD $= -0.93$; Mediated fraction $= 0.13/0.40 \approx 67\%$. Interventions are applied to the residual stream at Layer~11; readout is at the unembedding.}
    \label{fig:path-dag}
\end{figure}

\subsection*{Suppressors as forced solutions}
Across GPT-2 and Mistral we observe a common pattern: (i) \emph{function is conserved} (a circuit that implements a factuality–hedging trade-off), (ii) \emph{implementation adapts} (different heads and OV semantics), and (iii) \emph{location is forced} (layer~0 across both architectures). This ``conserved function, adapted implementation, forced location'' signature is exactly what a bottleneck-constrained solution predicts. The what is forced by the objective and geometry; the how is shaped by architecture and data.
