\section{Conclusion}

Layer-0 suppressors instantiate the statistical inevitability of hallucination at the circuit level.
By damping factual continuations and nudging models toward hedged discourse before higher layers act, they provide the mechanistic bridge between Kalai et al.'s theory and observed behavior.
Their presence across GPT-2 and Mistral, despite architectural differences, suggests suppressors are learned behavioral priors that gradient descent repeatedly rediscovers.

Because suppressors operate at the very start of the computation, downstream layers inherit the hedging mode and reinforce it, explaining why truthful answers remain elusive even when models possess the requisite knowledge.
Evaluation reform will be necessary to prevent new suppressors from forming, but existing models may also require direct circuit-level intervention.
Understanding, cataloguing, and steering suppressors therefore offers a promising path toward reducing hallucinations while preserving calibrated uncertainty.
