\section{Findings}\label{sec:findings}

\begin{table}[t]
    \centering
    \caption{Effect of layer-0 suppressor ablations on logit difference (LD). GPT-2 Medium: deterministic point estimates across three seeds (Apple M-series MPS). Mistral-7B: multi-seed H1 (3~seeds for negation/counterfactual); facts/logic use a single seed (seed~0) with collapsed CIs due to determinism on the 24-example splits. Positive $\Delta$LD indicates a stronger factual preference.}
    \label{tab:impact}
    \begin{tabular}{llcccc}
        \toprule
        Model & Task & Baseline LD & Suppressor ablated LD & $\Delta$LD & Heads \\
        \midrule
        GPT-2 Medium & Facts          & $1.484$ & $1.882$ & $+0.398$ & 0:2, 0:4, 0:7 \\
        GPT-2 Medium & Negation       & $1.607$ & $2.449$ & $+0.842$ & 0:2, 0:4, 0:7 \\
        GPT-2 Medium & Counterfactual & $1.420$ & $2.266$ & $+0.846$ & 0:2, 0:4, 0:7 \\
        GPT-2 Medium & Logic          & $1.294$ & $1.846$ & $+0.552$ & 0:2, 0:4, 0:7 \\
        \addlinespace
        Mistral 7B   & Facts          & $4.933$ & $4.930$ & $-0.003$ & 0:22, 0:23 \\
        Mistral 7B   & Negation       & $0.384$ & $0.609$ & $+0.225$ & 0:22, 0:23 \\
        Mistral 7B   & Counterfactual & $3.017$ & $3.299$ & $+0.282$ & 0:22, 0:23 \\
        Mistral 7B$^{\ddagger}$   & Logic          & $0.335$ & $0.293$ & $-0.042$ & 0:22, 0:23 \\
        \bottomrule
    \end{tabular}
    \vspace{0.5em}
    \footnotesize{$^{\ddagger}$Head~0:21 opposes heads~0:22/0:23 on the logic probe (net $\Delta$LD combines both effects).}
\end{table}

\paragraph{Prompt-composition robustness.}
Bootstrap over prompts (5k resamples) yields facts $\Delta$LD$=0.376$ [0.296,\,0.459], negation $=0.902$ [0.847,\,0.960], counterfactual $=0.744$ [0.540,\,0.959], logic $=0.545$ [0.504,\,0.584] for the layer-0 triplet $\{0{:}2,0{:}4,0{:}7\}$ ablation, mirroring the tabled effects and reducing sensitivity to prompt mix (Figure~\ref{fig:bootstrap-ci}).

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/bootstrap_ld_ci.pdf}
  \caption{\textbf{Prompt-robustness for GPT-2.} Bootstrap 95\% CIs (5k resamples) for $\Delta$LD across probe families under the L0 triplet ablation. Bars match the main table; CIs show stability to prompt composition.}
  \label{fig:bootstrap-ci}
\end{figure}

\subsection{Geometric signature: output flattening and trajectory bending}\label{sec:geom-signature}
We directly measured activation and output entropies, plus trajectory curvature, under baseline versus suppressor-ablated runs to test the information-theoretic account.

\paragraph{Output distribution flattening (strong).} With suppressors active the output distribution is dramatically flattened. On GPT-2 Medium (facts; $N=64$ prompts), last-token entropy drops from $6.44$~nats to $4.01$ when we ablate the layer-0 trio $(0{:}2,0{:}4,0{:}7)$, a change of $\Delta H = -2.44$~nats. Against 50 random layer-0 head sets of the same cardinality, none were more extreme in the predicted (negative) direction (randomisation test, $p < 0.02$, extreme tail). The same signature replicates across counterfactual ($\Delta H = -3.49$, $N=64$), negation ($\Delta H = -3.81$, $N=16$), and logic ($\Delta H = -3.08$, $N=16$) probes, each with 0/50 random controls at least as extreme. This confirms that suppressors systematically spread probability mass across hedging/meta-commentary tokens, preventing sharp factual predictions even when knowledge is available.

\paragraph{Trajectory curvature (supporting the attractor).} Measuring a simple discrete curvature proxy over layer-0 residuals across tokens, we see a consistent early-position decrease under ablation (facts: $\Delta \kappa_\text{early} = -14.6$; similar magnitudes on cf/neg/logic). This is consistent with an early hedging attractor: suppressors bend trajectories toward a stable region that downstream layers do not reverse; removing them straightens the path.

\paragraph{Activation geometry (expansion rather than compression).} Contrary to our initial compression prediction, activation entropies decrease under ablation across multiple estimators: full last-position MVN (facts $\Delta = -160.7$), diagonal (variance-only; $\Delta = -195.3$), and per-token averages (facts $\Delta = -203.8$); subspace entropies (PCA at 95\% variance) are small and mostly negative, occasionally near-zero or slightly positive depending on task. None of these deltas lie in an extreme tail versus random controls (two‑tailed $p\,\approx\,0.24$–$0.48$ across tasks/estimators). The consistent direction suggests suppressors \emph{expand} the activation cloud into a wide hedging region rather than compress it into a narrow subspace—while simultaneously flattening outputs. This rotation+expansion mechanism coheres with the curvature results and strengthens the bottleneck interpretation: at layer~0, suppressors allocate additional representational degrees of freedom to steer toward hedging, then distribute probability mass broadly at the output.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/geometric_signature.pdf}
  \caption{\textbf{Geometric signature of suppressors.} (Left) Distribution of output-entropy deltas from random layer‑0 head sets (gray); vertical lines show the observed deltas for facts/counterfactual/negation/logic under ablation of heads $(0{:}2,0{:}4,0{:}7)$—all lie in the extreme lower tail (sharper outputs). (Right) Early trajectory curvature deltas for the same four tasks (all negative), consistent with an early hedging attractor that disappears under ablation.}
  \label{fig:geom-signature}
\end{figure}


\subsection{Layer~0 as predicted: extreme-tail circuits at the first bottleneck}
Before zooming in on individual heads we measured geometry-level invariants.
Layer-wise activation patches (H2) reveal task-dependent phase shifts: GPT-2 Medium routes factual recall through layer~11, negation through layer~2, counterfactual reasoning through layer~8, and logic through layer~0.
Despite these shifts, three layer-0 heads---0:2, 0:4, and 0:7---retain high impact across all tasks with rank correlations $\rho \in [0.52,0.97]$ ($p \le 0.04$).
Rebalancing the corpora to equalise token frequencies \emph{increases} their prominence, indicating the signal is structural rather than a data artefact.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/random_l0_baseline.pdf}
    \caption{Distribution of $\Delta$LD for 1{,}000 random layer-0 ablations. Dotted and dash-dotted lines mark the 95th and 99th percentiles. Suppressor head~0:2 ($+0.398$) lies beyond the 99th percentile, and pairs $\{0{:}2,0{:}4\}$/$\{0{:}2,0{:}7\}$ land alongside the suppressor triplet $\{0{:}2,0{:}4,0{:}7\}$ in the extreme tail.}
    \label{fig:random-baseline}
\end{figure}

Figure~\ref{fig:random-baseline} shows head~0:2 producing $\Delta\mathrm{LD}=+0.398$, placing it at the very top of the single-head distribution. Heads~0:4 and 0:7 contribute $+0.130$ and $+0.124$, respectively—both around the 94th percentile while the random baseline’s 95th and 99th percentiles sit at $0.162$ and $0.169$. The suppressor pairs $(0{:}2,0{:}4)$ and $(0{:}2,0{:}7)$ deliver $+0.556$ and $+0.550$ LD shifts, placing them in the extreme tail of the simulated pair distribution (95th percentile $0.186$, 99th percentile $0.243$); the pair $(0{:}4,0{:}7)$ still exceeds the 99th percentile at $+0.253$. In parallel, information metrics (calibration) improve alongside power, passing our dual‑observable test for structural circuits.

\subsection{GPT-2 layer-0 suppressor}
\textit{A brief vignette.} When we ablated GPT-2 Medium’s layer-0 heads one by one, three stood out: 0:2, 0:4, and 0:7. Removing them made the model more factual, better calibrated, and less prone to “maybe” language. Statistically, these heads sat in the extreme tail of impact across tasks. In short: three heads that whisper \emph{maybe} before the model has even begun to reason.
Across all four probes the H1 heads-zero battery ranks layer-0 heads 2, 4, and~7 as the most damaging suppressors: ablation increases logit difference by 0.40--0.85 (Table~\ref{tab:impact}) and the trio sits at the top of the per-head tables in every condition.
The H5 triplet battery confirms destructive cooperation: pairwise ablations such as (0:2, 0:4) and (0:2, 0:7) raise logit difference nearly as much as removing all three, and the full triplet yields the largest gains (e.g., facts $+0.40$, negation $+0.84$).
H6 reverse patches show that pasting clean residuals into the corrupted run fails to restore factuality (facts $\Delta\mathrm{LD}=-0.048$), whereas the complementary clean$\rightarrow$corrupt patch reproduces suppression (H2 facts $\Delta\mathrm{LD}=+0.863$), indicating the circuit acts early and upstream.
OV projections reinforce the semantic interpretation: head~0:2 (and its partners) boost hedging/meta tokens such as \emph{perhaps}, \emph{maybe}, and \emph{seems} while suppressing factual continuations like \emph{Recomm}, \emph{trave}, and \emph{advoc}, demonstrating a coherent direction that trades factuality for hedging.
Lexicon enrichment (Appendix~\ref{app:lexicon}) quantifies this shift: head~0:2 shows log-odds enrichment of $+1.2$ for hedges and $+4.3$ for boosters relative to other layer-0 heads, whereas heads~0:4 and 0:7 show no enrichment, consistent with their secondary role.

\subsection*{Minimal intervention (OV-steer)}
Injecting $\alpha v$ at \verb|blocks.0.hook_resid_post| using the head~0:2 residual direction $v$ produces smooth, small changes in LD/ECE on \textsc{facts} as $\alpha$ sweeps $\{-0.5,0,+0.5\}$, with no collateral harm observed on \textsc{negation} in our micro-eval (Figure~\ref{fig:ov-steer}). This shows the suppressor direction is controllable at inference without degrading a non-target probe.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/ov_steer_demo.pdf}
  \caption{\textbf{Minimal intervention.} Steering with $\alpha v$ at layer 0 (head~0:2 residual direction) smoothly modulates LD and ECE on \textsc{facts} with negligible change on \textsc{negation}.}
  \label{fig:ov-steer}
\end{figure}

\paragraph{Span-aware effects (multi-token).}
To test whether suppressors only bias the \emph{first} next token or degrade entire answers, we added span-aware, teacher-forced sequence metrics (Methods): the sequence log-probability gap between target and foil ($\mathrm{seq\_logprob\_diff}$) and its drop under ablation ($\mathrm{seq\_p\_drop}$).
On GPT-2 Medium facts and counterfactual probes, the sequence metric mirrors first-token results (e.g., head~0:2: facts $\mathrm{seq\_logprob\_diff}=+1.46$; cf $+0.88$), confirming that the circuit degrades whole continuations.
On negation and logic the sequence effect is smaller—$14$--$22$\% of the first-token LD for head~0:2—revealing a clarifying dynamic: (i) suppressors act early and bias the first token strongly; (ii) downstream layers partially recover over longer spans when the sequence allows it; and (iii) different layer-0 heads matter for the recovery profile.
Indeed, the full-span ranking elevates heads~0:11/0:12/0:14 on negation/logic, which show editorial and adverbial OV directions without hedging-lexicon enrichment, in contrast to head~0:2’s hedging/booster signature.

\subsection{Mistral layer-0 suppressors}
On Mistral-7B the H1 battery flags layer-0 heads~22 and~23 as suppressors on counterfactual and negation probes, but the effect is task-contingent: facts show minimal change, and logic improves when either head is zeroed.
Replicating the H1 batteries at seeds~1 and~2 reproduces the seed~0 logit-difference trajectories to float-level precision (95\%~CI~$\approx 0$), so we continue to report the shared point estimates with a dagger in Table~\ref{tab:impact}.
H5 experiments isolate the causing pair: \{0:22, 0:23\} raises counterfactual logit difference by $+0.28$ and negation by $+0.23$ yet leaves facts flat ($+0.00$) and pushes logic down ($-0.04$).
The competition run reveals why logic behaves differently: head~0:21 alone produces a strong negative logit difference ($-0.39$), and pairing it with 0:22 overwhelms the suppressor effect.
Combined with the prior triplet runs, this indicates Mistral’s layer-0 houses both suppressive and anti-suppressive circuits, with head~21 opposing the \{22, 23\} pair on logical reasoning.
OV analysis corroborates the behavioral divergence: heads~22/23 suppress factual tokens (\emph{oppon, LIED, trag-}) without boosting hedging vocabulary, instead surfacing multilingual editorial fragments (\emph{acknow, départ, giornata}), so their direction lacks GPT-2’s hedging amplification.
 On logic, we therefore interpret head~0:21 as an \emph{anti-suppressor}: ablating it reduces logit-difference, suggesting a corrective circuit that restores factual preference against the 0:22/0:23 bias. This reflects modular competition, not a failure mode—opposing early-layer directions yield task-contingent net effects. Robustness then hinges on which circuit dominates under the prompt distribution.

\subsection{Scale robustness}
Layer-0 suppressors persist across GPT-2 scale. On GPT-2 Small (124M) the layer-0 heads {0:2, 0:4, 0:7} increase logit difference by $+0.38$, $+0.12$, and $+0.11$, respectively. GPT-2 Medium reproduces the same hierarchy with $+0.41$, $+0.13$, and $+0.12$, demonstrating that the circuit is architectural rather than a one-off checkpoint artifact. We report the Medium results in the main text to align with prior GPT-2-Medium analyses while noting that the motif already exists at smaller scale.
\subsection{Cross-model comparison}
Both models learn a layer-0 mechanism that degrades factual continuations, and ablations restore performance across multiple tasks, supporting the suppressor motif as a conserved behavioral prior.
Yet the implementations diverge: GPT-2’s trio jointly suppresses factuality and amplifies hedging, while Mistral’s pair suppresses factuality without a hedging boost and encounters opposition from a neighbouring head on logic.
The contrast suggests that although transformers converge on early suppressor behavior, the supporting circuitry adapts to architecture and training data, producing task-contingent variants rather than a single universal implementation.

\subsection{Learning dynamics on Pythia-160M}
To validate that suppressors are learned solutions rather than architectural artifacts or random initializations, we instrumented the Pythia-160M training trajectory via EleutherAI’s checkpoint tags and applied the H1 head-zero battery at nine checkpoints from initialisation ($0$) to $128$k steps (single seed; Pythia-tokenizer variants of our probe corpora).
Figure~\ref{fig:pythia-emergence} plots the head-wise ablation effect ($\Delta$LD) across training for layer-0 heads \{0:2, 0:4, 0:7, 0:11\} and all four probes.

Three findings close the loop with our prediction-first framing.
First, suppressors are \emph{learned, not hard-coded}. At step~0, head~0:2 yields weak/negative effects (e.g., facts $-0.23$, negation $-1.18$), but flips positive by $1$--$2$k steps and rises thereafter (peaks: facts $\approx 0.67$ @8k; neg/logic $\approx 2.7$ @16k; cf continues rising to $\approx 1.02$ @128k), consistent with an emergent learned solution.
Second, emergence is \emph{early and stabilising}: across probes, heads~0:2/0:4/0:7 enter the top-3 most damaging layer-0 heads by $2$k (neg/cf/logic) to $16$k (facts), then plateau or gently decline—a signature of gradient descent converging on a solution.
Third, \emph{task-contingency is real}. Head~0:11 (editorial/structural) trails the primary suppressors, entering the top-3 later on negation/logic (\~64k) while appearing earlier on facts/counterfactuals, consistent with the recovery-framing role we identified via span-aware analysis on GPT-2.

These learning dynamics close the loop with our prediction-first framing: layer-0 suppressors appear as soon as the model can exploit the bottleneck to implement the factuality--hedging trade-off, and later-acquired recovery heads refine behaviour over longer spans. Together with the GPT-2/Mistral results, this supports suppressors as learned, task-contingent solutions that self-organise at the first information bottleneck.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/pythia_emergence_curves.pdf}
    \caption{Pythia-160M learning dynamics. Emergence of layer-0 suppressor effects ($\Delta$LD for ablations) across checkpoints for heads 0:2/0:4/0:7/0:11 on all four probes. Curves rise from near-zero or negative at initialisation, plateau by mid training, and show task-contingent timing.}
    \label{fig:pythia-emergence}
\end{figure}
