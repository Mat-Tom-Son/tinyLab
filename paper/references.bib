@article{radford2019language,
  title   = {Language Models are Unsupervised Multitask Learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and others},
  journal = {OpenAI Technical Report},
  year    = {2019}
}

@article{jiang2023mistral,
  title   = {Mistral 7B},
  author  = {{Mistral AI}},
  journal = {arXiv preprint arXiv:2310.06825},
  year    = {2023}
}

@misc{tinylab2024,
  title        = {Tiny Ablation Lab Suppressor Studies},
  author       = {{Tiny Ablation Lab}},
  year         = {2024},
  howpublished = {Project documentation},
  url          = {https://example.com/tinylab}
}

@article{kalai2025why,
  title   = {Why Language Models Hallucinate},
  author  = {Kalai, Adam Tauman and Nachum, Ofir and Vempala, Santosh S. and Zhang, Eric},
  journal = {arXiv preprint arXiv:2509.04664},
  year    = {2025}
}

@inproceedings{aghajanyan2021better,
  title={Better Fine-Tuning by Reducing Representational Collapse},
  author={Aghajanyan, Armen and Shrivastava, Akshat and Gupta, Amal and Goyal, Naman and Zettlemoyer, Luke and Gupta, Sonal},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{elhage2021mathematical,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Yogatama, Dani and Brockman, Greg and Lieberman, Theodore and Amodei, Dario and others},
  journal={Transformer Circuits Thread},
  year={2021},
  howpublished={\url{https://transformer-circuits.pub/2021/framework}}
}

@article{bricken2023monosemantic,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Bricken, Trenton and Templeton, Alex and Batson, Joshua and Chen, Benjamin and Jermyn, Adam and Conerly, Toby and Olah, Chris},
  journal={Transformer Circuits Thread},
  year={2023},
  howpublished={\url{https://transformer-circuits.pub/2023/monosemantic-features}}
}

@inproceedings{hanna2023greater,
  title={How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Hanna, Michael and Liu, Ofir Press and Variengien, Aric},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{quirke2024understanding,
  title={Understanding Addition in Transformers},
  author={Quirke, Patrick and Barez, Filippo and Mendelsohn, Richard and Sheshadri, Arvind and Jermyn, Adam and Nanda, Neel},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{olsson2022incontext,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Goldie, Anna and others},
  journal={Transformer Circuits Thread},
  year={2022},
  howpublished={\url{https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads}}
}

@inproceedings{wang2023interpret,
  title={Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 small},
  author={Wang, Kevin and Variengien, Aric and Conmy, Alex and Shlegeris, Ben and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{mcdougall2024copy,
  title={Copy Suppression: Comprehensively Understanding an Attention Head},
  author={McDougall, Connor and Conmy, Alex and Rushing, Will and McGrath, Thomas and Nanda, Neel},
  booktitle={Proceedings of the 7th BlackboxNLP Workshop},
  year={2024}
}

@inproceedings{meng2022locating,
  title={Locating and Editing Factual Associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@misc{heimersheim2024activation,
  title={How to Use and Interpret Activation Patching},
  author={Heimersheim, S. and Nanda, N.},
  howpublished={Alignment Forum},
  year={2024},
  note={\url{https://www.alignmentforum.org/posts/}}
}

@article{kadavath2022language,
  title={Language Models (Mostly) Know What They Know},
  author={Kadavath, Saurav and Conerly, Toby and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nick and Jones, Andrew and Chen, Anna and Bai, Yuntao and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@inproceedings{lin2021truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{kalai2023calibrated,
  title={Calibrated Language Models Must Hallucinate},
  author={Kalai, Adam Tauman and Vempala, Santosh S.},
  journal={arXiv preprint arXiv:2311.14648},
  year={2023}
}

@inproceedings{valeriani2023geometry,
  title={Geometry of the Loss Landscape in Overparameterized Neural Networks},
  author={Valeriani, Daniele and Ciliberto, Carlo and Gales, Mark},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}


@article{olah2020zoom,
  title={Zoom In: An Introduction to Circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  year={2020},
  url={https://distill.pub/2020/circuits/zoom-in},
  note={https://distill.pub/2020/circuits/zoom-in}
}

@article{elhage2022toy,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tom and Olsson, Catherine and Schiefer, Nick and Henighan, Tom and Kravec, Scott and Chen, Catherine and Nanda, Neel and Joseph, Nicholas and Mann, Ben and others},
  journal={Transformer Circuits Thread},
  year={2022},
  howpublished={\url{https://transformer-circuits.pub/2022/toy_model}}
}
