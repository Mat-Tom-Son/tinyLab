@article{radford2019language,
  title   = {Language Models are Unsupervised Multitask Learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and others},
  journal = {OpenAI Technical Report},
  year    = {2019}
}

@article{jiang2023mistral,
  title   = {Mistral 7B},
  author  = {{Mistral AI}},
  journal = {arXiv preprint arXiv:2310.06825},
  year    = {2023}
}

@misc{tinylab2024,
  title        = {Tiny Ablation Lab Suppressor Studies},
  author       = {{Tiny Ablation Lab}},
  year         = {2024},
  howpublished = {Project documentation},
  url          = {https://example.com/tinylab}
}

@article{kalai2025why,
  title   = {Why Language Models Hallucinate},
  author  = {Kalai, Adam Tauman and Nachum, Ofir and Vempala, Santosh S. and Zhang, Eric},
  journal = {arXiv preprint arXiv:2509.04664},
  year    = {2025}
}

@inproceedings{aghajanyan2021better,
  title={Better Fine-Tuning by Reducing Representational Collapse},
  author={Aghajanyan, Armen and Shrivastava, Akshat and Gupta, Amal and Goyal, Naman and Zettlemoyer, Luke and Gupta, Sonal},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{elhage2021mathematical,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Yogatama, Dani and Brockman, Greg and Lieberman, Theodore and Amodei, Dario and others},
  journal={Transformer Circuits Thread},
  year={2021},
  howpublished={\url{https://transformer-circuits.pub/2021/framework}}
}

@article{bricken2023monosemantic,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Bricken, Trenton and Templeton, Alex and Batson, Joshua and Chen, Benjamin and Jermyn, Adam and Conerly, Toby and Olah, Chris},
  journal={Transformer Circuits Thread},
  year={2023},
  howpublished={\url{https://transformer-circuits.pub/2023/monosemantic-features}}
}

@inproceedings{hanna2023greater,
  title={How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Hanna, Michael and Liu, Ofir Press and Variengien, Aric},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{quirke2024understanding,
  title={Understanding Addition in Transformers},
  author={Quirke, Patrick and Barez, Filippo and Mendelsohn, Richard and Sheshadri, Arvind and Jermyn, Adam and Nanda, Neel},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{olsson2022incontext,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Goldie, Anna and others},
  journal={Transformer Circuits Thread},
  year={2022},
  howpublished={\url{https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads}}
}

@inproceedings{wang2023interpret,
  title={Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 small},
  author={Wang, Kevin and Variengien, Aric and Conmy, Alex and Shlegeris, Ben and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{mcdougall2024copy,
  title={Copy Suppression: Comprehensively Understanding an Attention Head},
  author={McDougall, Connor and Conmy, Alex and Rushing, Will and McGrath, Thomas and Nanda, Neel},
  booktitle={Proceedings of the 7th BlackboxNLP Workshop},
  year={2024}
}

@inproceedings{meng2022locating,
  title={Locating and Editing Factual Associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@misc{heimersheim2024activation,
  title={How to Use and Interpret Activation Patching},
  author={Heimersheim, S. and Nanda, N.},
  howpublished={Alignment Forum},
  year={2024},
  note={\url{https://www.alignmentforum.org/posts/}}
}

@article{kadavath2022language,
  title={Language Models (Mostly) Know What They Know},
  author={Kadavath, Saurav and Conerly, Toby and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nick and Jones, Andrew and Chen, Anna and Bai, Yuntao and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@inproceedings{lin2021truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{kalai2023calibrated,
  title={Calibrated Language Models Must Hallucinate},
  author={Kalai, Adam Tauman and Vempala, Santosh S.},
  journal={arXiv preprint arXiv:2311.14648},
  year={2023}
}

@inproceedings{valeriani2023geometry,
  title={Geometry of the Loss Landscape in Overparameterized Neural Networks},
  author={Valeriani, Daniele and Ciliberto, Carlo and Gales, Mark},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}


@article{olah2020zoom,
  title={Zoom In: An Introduction to Circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  year={2020},
  url={https://distill.pub/2020/circuits/zoom-in},
  note={https://distill.pub/2020/circuits/zoom-in}
}

@article{elhage2022toy,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tom and Olsson, Catherine and Schiefer, Nick and Henighan, Tom and Kravec, Scott and Chen, Catherine and Nanda, Neel and Joseph, Nicholas and Mann, Ben and others},
  journal={Transformer Circuits Thread},
  year={2022},
  howpublished={\url{https://transformer-circuits.pub/2022/toy_model}}
}

@misc{nanda2022transformerlens,
  title={TransformerLens: A Library for Mechanistic Interpretability of Generative Language Models},
  author={Nanda, Neel and Bloom, Joseph},
  year={2022},
  howpublished={\url{https://github.com/neelnanda-io/TransformerLens}}
}

@article{thompson2024mrc,
  title={Memory Resonance Condition in Transformers: A Null Result},
  author={Thompson, Mat},
  journal={arXiv preprint},
  year={2024},
  note={Previously published null result}
}

@article{kalai2025hallucination,
  title={Why Language Models Hallucinate and How to Reduce It},
  author={Kalai, Adam Tauman and Nachum, Ofir and Vempala, Santosh S.},
  journal={arXiv preprint arXiv:2501.XXXXX},
  year={2025}
}

@article{hanna2023gpt2greater,
  title={How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Hanna, Michael and Press, Ofir and Liu, Yann and Variengien, Aric},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@misc{heimersheim2024path,
  title={How to Use and Interpret Activation Patching},
  author={Heimersheim, Stefan and Nanda, Neel},
  howpublished={Alignment Forum},
  year={2024},
  url={https://www.alignmentforum.org/posts/}
}

@book{pearl2001direct,
  title={Direct and Indirect Effects},
  author={Pearl, Judea},
  booktitle={Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
  year={2001},
  publisher={Morgan Kaufmann}
}

@inproceedings{vig2020causal,
  title={Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Sakenis, Simas and Huang, Jason and Singer, Yaron and Shieber, Stuart},
  booktitle={arXiv preprint arXiv:2004.12265},
  year={2020}
}

@article{gundersen2018state,
  title={State of the Art: Reproducibility in Artificial Intelligence},
  author={Gundersen, Odd Erik and Kjensmo, Sigbj{\o}rn},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{lipton2019troubling,
  title={Troubling Trends in Machine Learning Scholarship},
  author={Lipton, Zachary C. and Steinhardt, Jacob},
  journal={Queue},
  volume={17},
  number={1},
  pages={45--77},
  year={2019},
  publisher={ACM}
}

@article{nosek2014registered,
  title={Registered Reports: A Method to Increase the Credibility of Published Results},
  author={Nosek, Brian A. and Lakens, Dani{\"e}l},
  journal={Social Psychology},
  volume={45},
  number={3},
  pages={137--141},
  year={2014}
}

@inproceedings{lucic2018gans,
  title={Are GANs Created Equal? A Large-Scale Study},
  author={Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@article{melis2018state,
  title={On the State of the Art of Evaluation in Neural Language Models},
  author={Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  journal={International Conference on Learning Representations},
  year={2018}
}

@article{jain2019attention,
  title={Attention is not Explanation},
  author={Jain, Sarthak and Wallace, Byron C.},
  journal={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2019}
}

@article{thompson2024entropy,
  title={Entropy Geometry and Information Flow in Transformers},
  author={Thompson, Mat},
  journal={arXiv preprint},
  year={2024},
  note={Previous work on information-theoretic analysis}
}

@article{bricken2023monosemanticity,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Bricken, Trenton and Templeton, Alex and Batson, Joshua and Chen, Benjamin and Jermyn, Adam and Conerly, Toby and Olah, Chris},
  journal={Transformer Circuits Thread},
  year={2023},
  howpublished={\url{https://transformer-circuits.pub/2023/monosemantic-features}}
}

@article{turner2023activation,
  title={Activation Addition: Steering Language Models Without Optimization},
  author={Turner, Alex and Thiergart, Lisa and Udell, David and Rauker, Tilman and Rein, David and Prakash, Neel and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}

@article{rafailov2023direct,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D. and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{hyland1998hedging,
  title={Hedging in Scientific Research Articles},
  author={Hyland, Ken},
  journal={Amsterdam: John Benjamins},
  year={1998}
}

@article{wang2023interpretability,
  title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={International Conference on Learning Representations},
  year={2023}
}

@article{olsson2022context,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Goldie, Anna and others},
  journal={Transformer Circuits Thread},
  year={2022},
  howpublished={\url{https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads}}
}
